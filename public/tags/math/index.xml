<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on Not A Rocket Scientist</title><link>http://localhost:1313/tags/math/</link><description>Recent content in Math on Not A Rocket Scientist</description><generator>Hugo</generator><language>en-gb</language><lastBuildDate>Mon, 10 Jul 2023 16:01:00 +0800</lastBuildDate><atom:link href="http://localhost:1313/tags/math/index.xml" rel="self" type="application/rss+xml"/><item><title>3D Visualisation of The Residual Sum of Squares</title><link>http://localhost:1313/posts/2023-07-10-3d-visualisation-of-the-residual-sum-of-squares/</link><pubDate>Mon, 10 Jul 2023 16:01:00 +0800</pubDate><guid>http://localhost:1313/posts/2023-07-10-3d-visualisation-of-the-residual-sum-of-squares/</guid><description>&lt;p>I recently came across the video &lt;a href="https://youtu.be/my3lsV-VQjs">The Beauty of Linear Regression (How to Fit a Line to your Data)&lt;/a> by &lt;a href="https://www.youtube.com/@RichBehiel">Richard Behiel&lt;/a>. The video is very inspiring! It illustrates the relationship between the residual sum of squares and the slope and intercept. The visualisation is intuitive and beautiful. I want to do a similar and simpler thing in the lecture slides for a course.&lt;/p>
&lt;p>In the video, the residual sum of squares was shown as a heatmap and the arrows were used to represent the negative gradients. Here I just want to use a Z-axis to represent the residual sum of squares, such that I have:&lt;/p></description></item><item><title>How Did Gauss Derive The Normal Distribution</title><link>http://localhost:1313/posts/2023-01-27-how-gauss-derived-the-normal-distribution/</link><pubDate>Fri, 27 Jan 2023 00:10:00 +0800</pubDate><guid>http://localhost:1313/posts/2023-01-27-how-gauss-derived-the-normal-distribution/</guid><description>&lt;p>I first came across with the normal distribution when I was 17 years old in high school. The &lt;strong>probability density function (PDF)&lt;/strong> is&lt;/p>
&lt;p>$$f_{\boldsymbol{X}}(x) = \cfrac{1}{\sqrt{2\pi}\sigma}\,e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$&lt;/p>
&lt;p>where $\mu$ and $\sigma$ are the population &lt;strong>mean&lt;/strong> and &lt;strong>standard deviation&lt;/strong>, respectively.&lt;/p>
&lt;p>Like most people when they first came across this function, the derivation of the function was not really introduced. Soon after that, we started to use it to calculate probabilities and other things. However, where does it come from? What&amp;rsquo;s the intuition of the function? Why are two of the most important constants $\pi$ and $e$ there? Those questions lingered in my brain ever since.&lt;/p></description></item></channel></rss>